{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f552a926",
   "metadata": {},
   "outputs": [],
   "source": [
    "COST_PER_MAIL = 0.50\n",
    "PROFIT_PER_SALE = 6.00\n",
    "BREAKEVEN = COST_PER_MAIL / PROFIT_PER_SALE  # 0.0833333333\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f4ea6921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (50000, 19)\n",
      "\n",
      "First 5 rows:\n",
      "   acctnum gender state    zip  zip3  first  last  book$  nonbook$  total$  \\\n",
      "0    10001      M    NY  10605   106     49    29    109       248     357   \n",
      "1    10002      M    NY  10960   109     39    27     35       103     138   \n",
      "2    10003      F    PA  19146   191     19    15     25       147     172   \n",
      "3    10004      F    NJ   7016    70      7     7     15       257     272   \n",
      "4    10005      F    NY  10804   108     15    15     15       134     149   \n",
      "\n",
      "   purch  child  youth  cook  do_it  refernce  art  geog  buyer  \n",
      "0     10      3      2     2      0         1    0     2      0  \n",
      "1      3      0      1     0      1         0    0     1      0  \n",
      "2      2      0      0     2      0         0    0     0      0  \n",
      "3      1      0      0     0      0         1    0     0      0  \n",
      "4      1      0      0     1      0         0    0     0      0  \n",
      "\n",
      "Summary statistics (last, purch, total$):\n",
      "               last        purch        total$\n",
      "count  50000.000000  50000.00000  50000.000000\n",
      "mean      12.358160      3.89022    208.318320\n",
      "std        8.153091      3.47627    101.357259\n",
      "min        1.000000      1.00000     15.000000\n",
      "25%        7.000000      1.00000    128.000000\n",
      "50%       11.000000      2.00000    209.000000\n",
      "75%       15.000000      6.00000    284.000000\n",
      "max       35.000000     12.00000    479.000000\n",
      "\n",
      "Overall buyer rate: 0.090\n",
      "\n",
      "Buyer rate by gender:\n",
      "gender\n",
      "F    0.071737\n",
      "M    0.127740\n",
      "Name: buyer, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Step 1.1\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"BBB.csv\")\n",
    "\n",
    "# Shape of the DataFrame\n",
    "print(\"Shape:\", df.shape)\n",
    "print()\n",
    "\n",
    "# First 5 rows\n",
    "print(\"First 5 rows:\")\n",
    "print(df.head())\n",
    "print()\n",
    "\n",
    "# Summary statistics for selected columns\n",
    "print(\"Summary statistics (last, purch, total$):\")\n",
    "print(df[[\"last\", \"purch\", \"total$\"]].describe())\n",
    "print()\n",
    "\n",
    "# Overall buyer rate\n",
    "overall_buyer_rate = df[\"buyer\"].mean()\n",
    "print(f\"Overall buyer rate: {overall_buyer_rate:.3f}\")\n",
    "print()\n",
    "\n",
    "# Buyer rate by gender\n",
    "buyer_rate_by_gender = df.groupby(\"gender\")[\"buyer\"].mean()\n",
    "print(\"Buyer rate by gender:\")\n",
    "print(buyer_rate_by_gender)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "14dd156b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  buyer   R-squared:                       0.015\n",
      "Model:                            OLS   Adj. R-squared:                  0.015\n",
      "Method:                 Least Squares   F-statistic:                     737.3\n",
      "Date:                Fri, 06 Feb 2026   Prob (F-statistic):          3.50e-161\n",
      "Time:                        01:10:38   Log-Likelihood:                -8134.4\n",
      "No. Observations:               50000   AIC:                         1.627e+04\n",
      "Df Residuals:                   49998   BIC:                         1.629e+04\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0517      0.002     27.079      0.000       0.048       0.055\n",
      "purch          0.0099      0.000     27.153      0.000       0.009       0.011\n",
      "==============================================================================\n",
      "Omnibus:                    27295.540   Durbin-Watson:                   1.995\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           140180.402\n",
      "Skew:                           2.796   Prob(JB):                         0.00\n",
      "Kurtosis:                       9.002   Cond. No.                         7.99\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "Min prediction: 0.062\n",
      "Max prediction: 0.171\n",
      "\n",
      "Share of predictions outside [0,1]: 0.000\n"
     ]
    }
   ],
   "source": [
    "#Step 1.2\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Load data (if not already loaded)\n",
    "df = pd.read_csv(\"BBB.csv\")\n",
    "\n",
    "# Define variables\n",
    "X = df[\"purch\"]\n",
    "y = df[\"buyer\"]\n",
    "\n",
    "# Add intercept\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit OLS\n",
    "ols_model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print regression summary\n",
    "print(ols_model.summary())\n",
    "print()\n",
    "\n",
    "# Generate predictions\n",
    "pred = ols_model.predict(X)\n",
    "\n",
    "# Min and max of predictions\n",
    "print(f\"Min prediction: {pred.min():.3f}\")\n",
    "print(f\"Max prediction: {pred.max():.3f}\")\n",
    "print()\n",
    "\n",
    "# Share of predictions outside [0, 1]\n",
    "outside_share = ((pred < 0) | (pred > 1)).mean()\n",
    "print(f\"Share of predictions outside [0,1]: {outside_share:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8e383483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic targeting results\n",
      "---------------------------\n",
      "# mailed: 17926\n",
      "# buyers mailed: 2346\n",
      "Cost: 8963.00\n",
      "Sales: 14076.00\n",
      "Profit: 5113.00\n",
      "ROME: 0.570\n",
      "\n",
      "Mass mailing results\n",
      "--------------------\n",
      "Profit (mass mailing): 2132.00\n"
     ]
    }
   ],
   "source": [
    "#Step 1.3\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"BBB.csv\")\n",
    "\n",
    "# Variables\n",
    "X = df[[\"purch\"]]   # sklearn expects 2D\n",
    "y = df[\"buyer\"]\n",
    "\n",
    "# Fit logistic regression\n",
    "logit = LogisticRegression(solver=\"lbfgs\")\n",
    "logit.fit(X, y)\n",
    "\n",
    "# Predicted probabilities\n",
    "df[\"p_hat\"] = logit.predict_proba(X)[:, 1]\n",
    "\n",
    "# Targeting cutoff\n",
    "cutoff = 0.0833\n",
    "\n",
    "# Targeted mailing\n",
    "mailed = df[\"p_hat\"] >= cutoff\n",
    "num_mailed = mailed.sum()\n",
    "buyers_mailed = df.loc[mailed, \"buyer\"].sum()\n",
    "\n",
    "cost = 0.50 * num_mailed\n",
    "sales = 6.0 * buyers_mailed\n",
    "profit = sales - cost\n",
    "rome = profit / cost\n",
    "\n",
    "print(\"Logistic targeting results\")\n",
    "print(\"---------------------------\")\n",
    "print(f\"# mailed: {num_mailed}\")\n",
    "print(f\"# buyers mailed: {buyers_mailed}\")\n",
    "print(f\"Cost: {cost:.2f}\")\n",
    "print(f\"Sales: {sales:.2f}\")\n",
    "print(f\"Profit: {profit:.2f}\")\n",
    "print(f\"ROME: {rome:.3f}\")\n",
    "print()\n",
    "\n",
    "# Mass mailing (mail everyone)\n",
    "total_mailed = len(df)\n",
    "total_buyers = df[\"buyer\"].sum()\n",
    "\n",
    "mass_cost = 0.50 * total_mailed\n",
    "mass_sales = 6.0 * total_buyers\n",
    "mass_profit = mass_sales - mass_cost\n",
    "\n",
    "print(\"Mass mailing results\")\n",
    "print(\"--------------------\")\n",
    "print(f\"Profit (mass mailing): {mass_profit:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2a74f2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                  buyer   No. Observations:                50000\n",
      "Model:                          Logit   Df Residuals:                    49995\n",
      "Method:                           MLE   Df Model:                            4\n",
      "Date:                Fri, 06 Feb 2026   Pseudo R-squ.:                 0.07194\n",
      "Time:                        01:10:38   Log-Likelihood:                -14086.\n",
      "converged:                       True   LL-Null:                       -15178.\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const          -2.2106      0.046    -48.268      0.000      -2.300      -2.121\n",
      "purch           0.0828      0.005     16.476      0.000       0.073       0.093\n",
      "last           -0.0827      0.003    -32.115      0.000      -0.088      -0.078\n",
      "total_spent     0.0009      0.000      4.907      0.000       0.001       0.001\n",
      "gender_M        0.5096      0.033     15.544      0.000       0.445       0.574\n",
      "===============================================================================\n",
      "\n",
      "Multi-variable logistic targeting results\n",
      "-----------------------------------------\n",
      "# mailed: 22376\n",
      "# buyers mailed: 3312\n",
      "Cost: 11188.00\n",
      "Sales: 19872.00\n",
      "Profit: 8684.00\n",
      "ROME: 0.776\n"
     ]
    }
   ],
   "source": [
    "#Step 1.4\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"BBB.csv\")\n",
    "\n",
    "# --------------------\n",
    "# Feature preparation\n",
    "# --------------------\n",
    "\n",
    "# Rename total$\n",
    "df = df.rename(columns={\"total$\": \"total_spent\"})\n",
    "\n",
    "# One-hot encode gender, drop first category\n",
    "df = pd.get_dummies(df, columns=[\"gender\"], drop_first=True)\n",
    "\n",
    "# Convert boolean dummies to int (0/1), if any\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == bool:\n",
    "        df[col] = df[col].astype(int)\n",
    "\n",
    "# Define variables\n",
    "features = [\"purch\", \"last\", \"total_spent\"] + \\\n",
    "           [c for c in df.columns if c.startswith(\"gender_\")]\n",
    "\n",
    "X = df[features]\n",
    "y = df[\"buyer\"]\n",
    "\n",
    "# --------------------\n",
    "# statsmodels Logit\n",
    "# --------------------\n",
    "\n",
    "X_sm = sm.add_constant(X)\n",
    "logit_sm = sm.Logit(y, X_sm).fit(disp=False)\n",
    "\n",
    "print(logit_sm.summary())\n",
    "print()\n",
    "\n",
    "# --------------------\n",
    "# sklearn Logit for predict_proba\n",
    "# --------------------\n",
    "\n",
    "logit_sk = LogisticRegression(solver=\"lbfgs\", max_iter=1000)\n",
    "logit_sk.fit(X, y)\n",
    "\n",
    "df[\"p_hat\"] = logit_sk.predict_proba(X)[:, 1]\n",
    "\n",
    "# --------------------\n",
    "# Targeting at cutoff\n",
    "# --------------------\n",
    "\n",
    "cutoff = 0.0833\n",
    "mailed = df[\"p_hat\"] >= cutoff\n",
    "\n",
    "num_mailed = mailed.sum()\n",
    "buyers_mailed = df.loc[mailed, \"buyer\"].sum()\n",
    "\n",
    "cost = 0.50 * num_mailed\n",
    "sales = 6.0 * buyers_mailed\n",
    "profit = sales - cost\n",
    "rome = profit / cost\n",
    "\n",
    "print(\"Multi-variable logistic targeting results\")\n",
    "print(\"-----------------------------------------\")\n",
    "print(f\"# mailed: {num_mailed}\")\n",
    "print(f\"# buyers mailed: {buyers_mailed}\")\n",
    "print(f\"Cost: {cost:.2f}\")\n",
    "print(f\"Sales: {sales:.2f}\")\n",
    "print(f\"Profit: {profit:.2f}\")\n",
    "print(f\"ROME: {rome:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "fe7c8216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                  buyer   No. Observations:                50000\n",
      "Model:                          Logit   Df Residuals:                    49993\n",
      "Method:                           MLE   Df Model:                            6\n",
      "Date:                Fri, 06 Feb 2026   Pseudo R-squ.:                  0.1252\n",
      "Time:                        01:10:38   Log-Likelihood:                -13277.\n",
      "converged:                       True   LL-Null:                       -15178.\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "=====================================================================================\n",
      "                        coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "Intercept            -2.5759      0.052    -49.988      0.000      -2.677      -2.475\n",
      "purch                -0.0027      0.006     -0.484      0.628      -0.014       0.008\n",
      "last                 -0.0851      0.003    -32.415      0.000      -0.090      -0.080\n",
      "total_spent           0.0010      0.000      5.130      0.000       0.001       0.001\n",
      "gender_M              0.8238      0.050     16.422      0.000       0.725       0.922\n",
      "art_flag              1.6844      0.048     35.100      0.000       1.590       1.778\n",
      "gender_M:art_flag    -0.5549      0.066     -8.391      0.000      -0.685      -0.425\n",
      "=====================================================================================\n",
      "\n",
      "Interaction model targeting results\n",
      "-----------------------------------\n",
      "# mailed: 17296\n",
      "# buyers mailed: 3225\n",
      "Profit: 10702.00\n",
      "ROME: 1.238\n"
     ]
    }
   ],
   "source": [
    "#Step 1.5\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"BBB.csv\")\n",
    "\n",
    "# --------------------\n",
    "# Feature preparation\n",
    "# --------------------\n",
    "\n",
    "# Rename total$\n",
    "df = df.rename(columns={\"total$\": \"total_spent\"})\n",
    "\n",
    "# One-hot encode gender, keep explicit dummy name (e.g., gender_M)\n",
    "df = pd.get_dummies(df, columns=[\"gender\"], drop_first=True)\n",
    "\n",
    "# Convert boolean dummies to int\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == bool:\n",
    "        df[col] = df[col].astype(int)\n",
    "\n",
    "# Create art_flag if art is a count\n",
    "df[\"art_flag\"] = (df[\"art\"] > 0).astype(int)\n",
    "\n",
    "# --------------------\n",
    "# statsmodels Logit with interaction\n",
    "# --------------------\n",
    "\n",
    "# Formula with interaction (includes main effects automatically)\n",
    "formula = (\n",
    "    \"buyer ~ purch + last + total_spent + gender_M + art_flag + gender_M * art_flag\"\n",
    ")\n",
    "\n",
    "logit_model = smf.logit(formula=formula, data=df).fit(disp=False)\n",
    "\n",
    "print(logit_model.summary())\n",
    "print()\n",
    "\n",
    "# --------------------\n",
    "# Predictions & targeting\n",
    "# --------------------\n",
    "\n",
    "df[\"p_hat\"] = logit_model.predict(df)\n",
    "\n",
    "cutoff = 0.0833\n",
    "mailed = df[\"p_hat\"] >= cutoff\n",
    "\n",
    "num_mailed = mailed.sum()\n",
    "buyers_mailed = df.loc[mailed, \"buyer\"].sum()\n",
    "\n",
    "cost = 0.50 * num_mailed\n",
    "sales = 6.0 * buyers_mailed\n",
    "profit = sales - cost\n",
    "rome = profit / cost\n",
    "\n",
    "print(\"Interaction model targeting results\")\n",
    "print(\"-----------------------------------\")\n",
    "print(f\"# mailed: {num_mailed}\")\n",
    "print(f\"# buyers mailed: {buyers_mailed}\")\n",
    "print(f\"Profit: {profit:.2f}\")\n",
    "print(f\"ROME: {rome:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a3f8e7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 feature importances:\n",
      "art         0.594866\n",
      "last        0.242530\n",
      "geog        0.092794\n",
      "gender_M    0.031559\n",
      "do_it       0.029275\n",
      "cook        0.006601\n",
      "nonbook$    0.002374\n",
      "state_RI    0.000000\n",
      "state_PA    0.000000\n",
      "state_MD    0.000000\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 1.6\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# --------------------\n",
    "# Load data\n",
    "# --------------------\n",
    "df = pd.read_csv(\"BBB.csv\")\n",
    "\n",
    "# Rename total$ if needed\n",
    "if \"total$\" in df.columns:\n",
    "    df = df.rename(columns={\"total$\": \"total_spent\"})\n",
    "\n",
    "# --------------------\n",
    "# Feature matrix\n",
    "# --------------------\n",
    "features = [\n",
    "    \"state\", \"first\", \"last\", \"book$\", \"nonbook$\", \"total_spent\",\n",
    "    \"purch\", \"child\", \"youth\", \"cook\", \"do_it\", \"refernce\",\n",
    "    \"art\", \"geog\", \"gender\"\n",
    "]\n",
    "\n",
    "X = df[features]\n",
    "y = df[\"buyer\"]\n",
    "\n",
    "# One-hot encode state and gender\n",
    "X = pd.get_dummies(X, columns=[\"state\", \"gender\"], drop_first=True)\n",
    "\n",
    "# Ensure all features are numeric\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == bool:\n",
    "        X[col] = X[col].astype(int)\n",
    "\n",
    "# --------------------\n",
    "# Train / test split\n",
    "# --------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.30,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# --------------------\n",
    "# Decision Tree\n",
    "# --------------------\n",
    "tree = DecisionTreeClassifier(\n",
    "    max_depth=5,\n",
    "    min_samples_leaf=100,\n",
    "    random_state=42\n",
    ")\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# --------------------\n",
    "# Feature importances\n",
    "# --------------------\n",
    "importances = pd.Series(tree.feature_importances_, index=X.columns)\n",
    "top10 = importances.sort_values(ascending=False).head(10)\n",
    "\n",
    "print(\"Top 10 feature importances:\")\n",
    "print(top10)\n",
    "\n",
    "# --------------------\n",
    "# Plot (horizontal bar chart)\n",
    "# --------------------\n",
    "plt.figure()\n",
    "top10.sort_values().plot(kind=\"barh\")\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.title(\"Top 10 Decision Tree Feature Importances\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "dd077214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Strategy  # Mailed  # Buyers Mailed   Profit      ROME\n",
      "0        Mass mailing     50000             4522   2132.0  0.085280\n",
      "1        Logit: purch     17926             2346   5113.0  0.570456\n",
      "2    Logit: multi-var     22376             3312   8684.0  0.776189\n",
      "3  Logit: interaction     17296             3225  10702.0  1.237512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe interaction-based logistic regression gives the highest profit because it uses the most information \\nand can better identify customers who are truly likely to buy, including differences in preferences \\nacross groups. The most efficient approach (highest ROME) is usually a simpler targeted logistic model, \\nwhich mails to fewer people and keeps costs very low while still generating sales. Overall, more personalized, \\ndata-driven methods outperform simpler approaches because they account for differences in customer behavior, \\nreduce wasted mailings, and focus marketing resources on the most promising customers \\nrather than treating everyone the same.\\n'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 1.7\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# --------------------\n",
    "# Setup\n",
    "# --------------------\n",
    "df = pd.read_csv(\"BBB.csv\")\n",
    "\n",
    "# Rename total$ if needed\n",
    "if \"total$\" in df.columns:\n",
    "    df = df.rename(columns={\"total$\": \"total_spent\"})\n",
    "\n",
    "# Gender dummy\n",
    "df = pd.get_dummies(df, columns=[\"gender\"], drop_first=True)\n",
    "\n",
    "# Boolean  int\n",
    "for c in df.columns:\n",
    "    if df[c].dtype == bool:\n",
    "        df[c] = df[c].astype(int)\n",
    "\n",
    "# Art flag\n",
    "df[\"art_flag\"] = (df[\"art\"] > 0).astype(int)\n",
    "\n",
    "cutoff = 0.0833\n",
    "cost_per_mail = 0.50\n",
    "revenue_per_sale = 6.0\n",
    "\n",
    "results = []\n",
    "\n",
    "# --------------------\n",
    "# 1. Mass mailing\n",
    "# --------------------\n",
    "mailed = len(df)\n",
    "buyers_mailed = df[\"buyer\"].sum()\n",
    "\n",
    "cost = cost_per_mail * mailed\n",
    "sales = revenue_per_sale * buyers_mailed\n",
    "profit = sales - cost\n",
    "rome = profit / cost\n",
    "\n",
    "results.append([\n",
    "    \"Mass mailing\", mailed, buyers_mailed, profit, rome\n",
    "])\n",
    "\n",
    "# --------------------\n",
    "# 2. Logit: purch only\n",
    "# --------------------\n",
    "X = df[[\"purch\"]]\n",
    "y = df[\"buyer\"]\n",
    "\n",
    "logit1 = LogisticRegression(max_iter=1000)\n",
    "logit1.fit(X, y)\n",
    "\n",
    "df[\"p1\"] = logit1.predict_proba(X)[:, 1]\n",
    "mailed_mask = df[\"p1\"] >= cutoff\n",
    "\n",
    "mailed = mailed_mask.sum()\n",
    "buyers_mailed = df.loc[mailed_mask, \"buyer\"].sum()\n",
    "\n",
    "cost = cost_per_mail * mailed\n",
    "sales = revenue_per_sale * buyers_mailed\n",
    "profit = sales - cost\n",
    "rome = profit / cost\n",
    "\n",
    "results.append([\n",
    "    \"Logit: purch\", mailed, buyers_mailed, profit, rome\n",
    "])\n",
    "\n",
    "# --------------------\n",
    "# 3. Logit: purch + last + total_spent + gender\n",
    "# --------------------\n",
    "features = [\"purch\", \"last\", \"total_spent\"] + \\\n",
    "           [c for c in df.columns if c.startswith(\"gender_\")]\n",
    "\n",
    "X = df[features]\n",
    "\n",
    "logit2 = LogisticRegression(max_iter=1000)\n",
    "logit2.fit(X, y)\n",
    "\n",
    "df[\"p2\"] = logit2.predict_proba(X)[:, 1]\n",
    "mailed_mask = df[\"p2\"] >= cutoff\n",
    "\n",
    "mailed = mailed_mask.sum()\n",
    "buyers_mailed = df.loc[mailed_mask, \"buyer\"].sum()\n",
    "\n",
    "cost = cost_per_mail * mailed\n",
    "sales = revenue_per_sale * buyers_mailed\n",
    "profit = sales - cost\n",
    "rome = profit / cost\n",
    "\n",
    "results.append([\n",
    "    \"Logit: multi-var\", mailed, buyers_mailed, profit, rome\n",
    "])\n",
    "\n",
    "# --------------------\n",
    "# 4. Logit with interaction: gender  art_flag\n",
    "# --------------------\n",
    "formula = (\n",
    "    \"buyer ~ purch + last + total_spent + gender_M + art_flag + gender_M * art_flag\"\n",
    ")\n",
    "\n",
    "logit_int = smf.logit(formula, data=df).fit(disp=False)\n",
    "df[\"p3\"] = logit_int.predict(df)\n",
    "\n",
    "mailed_mask = df[\"p3\"] >= cutoff\n",
    "mailed = mailed_mask.sum()\n",
    "buyers_mailed = df.loc[mailed_mask, \"buyer\"].sum()\n",
    "\n",
    "cost = cost_per_mail * mailed\n",
    "sales = revenue_per_sale * buyers_mailed\n",
    "profit = sales - cost\n",
    "rome = profit / cost\n",
    "\n",
    "results.append([\n",
    "    \"Logit: interaction\", mailed, buyers_mailed, profit, rome\n",
    "])\n",
    "\n",
    "# --------------------\n",
    "# Summary table\n",
    "# --------------------\n",
    "summary = pd.DataFrame(\n",
    "    results,\n",
    "    columns=[\"Strategy\", \"# Mailed\", \"# Buyers Mailed\", \"Profit\", \"ROME\"]\n",
    ")\n",
    "\n",
    "print(summary)\n",
    "\n",
    "\n",
    "'''\n",
    "The interaction-based logistic regression gives the highest profit because it uses the most information \n",
    "and can better identify customers who are truly likely to buy, including differences in preferences \n",
    "across groups. The most efficient approach (highest ROME) is usually a simpler targeted logistic model, \n",
    "which mails to fewer people and keeps costs very low while still generating sales. Overall, more personalized, \n",
    "data-driven methods outperform simpler approaches because they account for differences in customer behavior, \n",
    "reduce wasted mailings, and focus marketing resources on the most promising customers \n",
    "rather than treating everyone the same.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "88972c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBased on the decision tree importances and consumer behavior logic, a strong refined logistic model would include purch, last, \\ntotal_spent, art, gender, and first.\\n\\nPurch (number of past purchases) reflects customer loyalty and habit formationcustomers who have bought more often in the past are more likely to buy again.\\n\\nLast (recency of last purchase)** captures engagement; more recent purchasers are still active and responsive to marketing.\\n\\nTotal_spent measures customer value and willingness to pay, indicating higher purchasing capacity and interest in books overall.\\n\\nArt (or art_flag) signals topical affinity; customers who previously purchased art-related items are more likely to buy a Florence-related book.\\n\\nGender is relevant because preferences for certain book categories differ systematically across genders in the data.\\n\\nFirst (tenure) captures relationship lengthlonger-tenured customers may trust the seller more and be more responsive to targeted offers.\\n\\nTogether, these variables combine behavioral history, engagement, value, and preference alignment*, making them well-suited for predicting purchase of the Florence book.\\n'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 2.A\n",
    "\n",
    "'''\n",
    "Based on the decision tree importances and consumer behavior logic, a strong refined logistic model would include purch, last, \n",
    "total_spent, art, gender, and first.\n",
    "\n",
    "Purch (number of past purchases) reflects customer loyalty and habit formationcustomers who have bought more often in the past are more likely to buy again.\n",
    "\n",
    "Last (recency of last purchase)** captures engagement; more recent purchasers are still active and responsive to marketing.\n",
    "\n",
    "Total_spent measures customer value and willingness to pay, indicating higher purchasing capacity and interest in books overall.\n",
    "\n",
    "Art (or art_flag) signals topical affinity; customers who previously purchased art-related items are more likely to buy a Florence-related book.\n",
    "\n",
    "Gender is relevant because preferences for certain book categories differ systematically across genders in the data.\n",
    "\n",
    "First (tenure) captures relationship lengthlonger-tenured customers may trust the seller more and be more responsive to targeted offers.\n",
    "\n",
    "Together, these variables combine behavioral history, engagement, value, and preference alignment*, making them well-suited for predicting purchase of the Florence book.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ca06d33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                  buyer   No. Observations:                50000\n",
      "Model:                          Logit   Df Residuals:                    49993\n",
      "Method:                           MLE   Df Model:                            6\n",
      "Date:                Fri, 06 Feb 2026   Pseudo R-squ.:                  0.1252\n",
      "Time:                        01:10:39   Log-Likelihood:                -13277.\n",
      "converged:                       True   LL-Null:                       -15178.\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "=====================================================================================\n",
      "                        coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "Intercept            -2.5759      0.052    -49.988      0.000      -2.677      -2.475\n",
      "purch                -0.0027      0.006     -0.484      0.628      -0.014       0.008\n",
      "last                 -0.0851      0.003    -32.415      0.000      -0.090      -0.080\n",
      "total_spent           0.0010      0.000      5.130      0.000       0.001       0.001\n",
      "gender_M              0.8238      0.050     16.422      0.000       0.725       0.922\n",
      "art_flag              1.6844      0.048     35.100      0.000       1.590       1.778\n",
      "gender_M:art_flag    -0.5549      0.066     -8.391      0.000      -0.685      -0.425\n",
      "=====================================================================================\n"
     ]
    }
   ],
   "source": [
    "#Step 2.B\n",
    "\n",
    "'''The gender_M  art_flag interaction allows the model to capture that art interest may affect \n",
    "purchase likelihood differently for men and women, rather than assuming the same effect across genders.\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"BBB.csv\")\n",
    "\n",
    "# Rename total$ if needed\n",
    "if \"total$\" in df.columns:\n",
    "    df = df.rename(columns={\"total$\": \"total_spent\"})\n",
    "\n",
    "# One-hot encode gender\n",
    "df = pd.get_dummies(df, columns=[\"gender\"], drop_first=True)\n",
    "\n",
    "# Convert booleans to ints\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == bool:\n",
    "        df[col] = df[col].astype(int)\n",
    "\n",
    "# Create art_flag\n",
    "df[\"art_flag\"] = (df[\"art\"] > 0).astype(int)\n",
    "\n",
    "# Logistic regression with ONE interaction\n",
    "formula = (\n",
    "    \"buyer ~ purch + last + total_spent + gender_M + art_flag + gender_M * art_flag\"\n",
    ")\n",
    "\n",
    "logit_model = smf.logit(formula=formula, data=df).fit(disp=False)\n",
    "\n",
    "# Print summary\n",
    "print(logit_model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2514234e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                  buyer   No. Observations:                50000\n",
      "Model:                          Logit   Df Residuals:                    49993\n",
      "Method:                           MLE   Df Model:                            6\n",
      "Date:                Fri, 06 Feb 2026   Pseudo R-squ.:                  0.1252\n",
      "Time:                        01:10:40   Log-Likelihood:                -13277.\n",
      "converged:                       True   LL-Null:                       -15178.\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "=====================================================================================\n",
      "                        coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "Intercept            -2.5759      0.052    -49.988      0.000      -2.677      -2.475\n",
      "purch                -0.0027      0.006     -0.484      0.628      -0.014       0.008\n",
      "last                 -0.0851      0.003    -32.415      0.000      -0.090      -0.080\n",
      "total_spent           0.0010      0.000      5.130      0.000       0.001       0.001\n",
      "gender_M              0.8238      0.050     16.422      0.000       0.725       0.922\n",
      "art_flag              1.6844      0.048     35.100      0.000       1.590       1.778\n",
      "gender_M:art_flag    -0.5549      0.066     -8.391      0.000      -0.685      -0.425\n",
      "=====================================================================================\n",
      "\n",
      "Campaign results (interaction model)\n",
      "------------------------------------\n",
      "# mailed: 17040\n",
      "# buyers mailed: 3196\n",
      "Profit: 10656.00\n",
      "ROME: 1.251\n"
     ]
    }
   ],
   "source": [
    "#Step 2.C\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# --------------------\n",
    "# Load and prepare data\n",
    "# --------------------\n",
    "df = pd.read_csv(\"BBB.csv\")\n",
    "\n",
    "# Rename total$ if needed\n",
    "if \"total$\" in df.columns:\n",
    "    df = df.rename(columns={\"total$\": \"total_spent\"})\n",
    "\n",
    "# Gender dummy\n",
    "df = pd.get_dummies(df, columns=[\"gender\"], drop_first=True)\n",
    "\n",
    "# Convert booleans to int\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == bool:\n",
    "        df[col] = df[col].astype(int)\n",
    "\n",
    "# Art flag\n",
    "df[\"art_flag\"] = (df[\"art\"] > 0).astype(int)\n",
    "\n",
    "# --------------------\n",
    "# statsmodels Logit (with interaction)\n",
    "# --------------------\n",
    "formula = (\n",
    "    \"buyer ~ purch + last + total_spent + gender_M + art_flag + gender_M * art_flag\"\n",
    ")\n",
    "\n",
    "logit_sm = smf.logit(formula=formula, data=df).fit(disp=False)\n",
    "print(logit_sm.summary())\n",
    "print()\n",
    "\n",
    "# --------------------\n",
    "# sklearn Logit for predict_proba\n",
    "# --------------------\n",
    "features = [\"purch\", \"last\", \"total_spent\", \"gender_M\", \"art_flag\"]\n",
    "X = df[features]\n",
    "y = df[\"buyer\"]\n",
    "\n",
    "logit_sk = LogisticRegression(max_iter=1000)\n",
    "logit_sk.fit(X, y)\n",
    "\n",
    "df[\"p_hat\"] = logit_sk.predict_proba(X)[:, 1]\n",
    "\n",
    "# --------------------\n",
    "# Targeting & economics\n",
    "# --------------------\n",
    "cutoff = 0.0833\n",
    "cost_per_mail = 0.50\n",
    "revenue_per_sale = 6.0\n",
    "\n",
    "mailed = df[\"p_hat\"] >= cutoff\n",
    "num_mailed = mailed.sum()\n",
    "buyers_mailed = df.loc[mailed, \"buyer\"].sum()\n",
    "\n",
    "cost = cost_per_mail * num_mailed\n",
    "sales = revenue_per_sale * buyers_mailed\n",
    "profit = sales - cost\n",
    "rome = profit / cost\n",
    "\n",
    "print(\"Campaign results (interaction model)\")\n",
    "print(\"------------------------------------\")\n",
    "print(f\"# mailed: {num_mailed}\")\n",
    "print(f\"# buyers mailed: {buyers_mailed}\")\n",
    "print(f\"Profit: {profit:.2f}\")\n",
    "print(f\"ROME: {rome:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "624d104a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFM Targeting Results\n",
      "---------------------\n",
      "# mailed: 24364\n",
      "# buyers mailed: 3349\n",
      "Profit: 7912.00\n",
      "ROME: 0.649\n"
     ]
    }
   ],
   "source": [
    "#Step 2.D\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --------------------\n",
    "# Load data\n",
    "# --------------------\n",
    "df = pd.read_csv(\"BBB.csv\")\n",
    "\n",
    "# Rename total$ if needed\n",
    "if \"total$\" in df.columns:\n",
    "    df = df.rename(columns={\"total$\": \"total_spent\"})\n",
    "\n",
    "# --------------------\n",
    "# Create RFM quintiles\n",
    "# --------------------\n",
    "df[\"R\"] = pd.qcut(df[\"last\"].rank(method=\"first\"), 5, labels=False)\n",
    "df[\"F\"] = pd.qcut(df[\"purch\"].rank(method=\"first\"), 5, labels=False)\n",
    "df[\"M\"] = pd.qcut(df[\"total_spent\"].rank(method=\"first\"), 5, labels=False)\n",
    "\n",
    "# Concatenate RFM code\n",
    "df[\"RFM\"] = df[\"R\"].astype(str) + df[\"F\"].astype(str) + df[\"M\"].astype(str)\n",
    "\n",
    "# --------------------\n",
    "# Cell-level economics\n",
    "# --------------------\n",
    "rfm_summary = (\n",
    "    df.groupby(\"RFM\")\n",
    "      .agg(\n",
    "          total_customers=(\"buyer\", \"count\"),\n",
    "          total_buyers=(\"buyer\", \"sum\")\n",
    "      )\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "rfm_summary[\"cost\"] = 0.50 * rfm_summary[\"total_customers\"]\n",
    "rfm_summary[\"sales\"] = 6.0 * rfm_summary[\"total_buyers\"]\n",
    "rfm_summary[\"profit\"] = rfm_summary[\"sales\"] - rfm_summary[\"cost\"]\n",
    "\n",
    "# --------------------\n",
    "# Target only profitable cells\n",
    "# --------------------\n",
    "profitable_cells = rfm_summary[rfm_summary[\"profit\"] > 0]\n",
    "\n",
    "mailed = profitable_cells[\"total_customers\"].sum()\n",
    "buyers_mailed = profitable_cells[\"total_buyers\"].sum()\n",
    "\n",
    "cost = 0.50 * mailed\n",
    "sales = 6.0 * buyers_mailed\n",
    "profit = sales - cost\n",
    "rome = profit / cost\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# Report results\n",
    "# --------------------\n",
    "print(\"RFM Targeting Results\")\n",
    "print(\"---------------------\")\n",
    "print(f\"# mailed: {mailed}\")\n",
    "print(f\"# buyers mailed: {buyers_mailed}\")\n",
    "print(f\"Profit: {profit:.2f}\")\n",
    "print(f\"ROME: {rome:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "3fee6277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Strategy  # Mailed  \\\n",
      "0                                      RFM targeting     24364   \n",
      "1  Logit baseline (purch + last + total_spent + g...     22376   \n",
      "2                  Enhanced logit (with interaction)     17296   \n",
      "\n",
      "   # Buyers Mailed   Profit      ROME  \n",
      "0             3349   7912.0  0.649483  \n",
      "1             3312   8684.0  0.776189  \n",
      "2             3225  10702.0  1.237512  \n"
     ]
    }
   ],
   "source": [
    "#Step 2.E\n",
    "\n",
    "'''\n",
    "The enhanced logistic model with one interaction delivers the highest total profit, while the same enhanced model \n",
    "(or occasionally the baseline logit) achieves the highest ROME, outperforming RFM by using continuous predictors \n",
    "and economically meaningful heterogeneity rather than coarse segment averages.\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# --------------------\n",
    "# Load & prep data\n",
    "# --------------------\n",
    "df = pd.read_csv(\"BBB.csv\")\n",
    "\n",
    "if \"total$\" in df.columns:\n",
    "    df = df.rename(columns={\"total$\": \"total_spent\"})\n",
    "\n",
    "df = pd.get_dummies(df, columns=[\"gender\"], drop_first=True)\n",
    "\n",
    "for c in df.columns:\n",
    "    if df[c].dtype == bool:\n",
    "        df[c] = df[c].astype(int)\n",
    "\n",
    "df[\"art_flag\"] = (df[\"art\"] > 0).astype(int)\n",
    "\n",
    "cutoff = 0.0833\n",
    "cost_per_mail = 0.50\n",
    "revenue_per_sale = 6.0\n",
    "\n",
    "# --------------------\n",
    "# (1) RFM TARGETING\n",
    "# --------------------\n",
    "df[\"R\"] = pd.qcut(df[\"last\"].rank(method=\"first\"), 5, labels=False)\n",
    "df[\"F\"] = pd.qcut(df[\"purch\"].rank(method=\"first\"), 5, labels=False)\n",
    "df[\"M\"] = pd.qcut(df[\"total_spent\"].rank(method=\"first\"), 5, labels=False)\n",
    "\n",
    "df[\"RFM\"] = df[\"R\"].astype(str) + df[\"F\"].astype(str) + df[\"M\"].astype(str)\n",
    "\n",
    "rfm = (\n",
    "    df.groupby(\"RFM\")\n",
    "      .agg(customers=(\"buyer\", \"count\"), buyers=(\"buyer\", \"sum\"))\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "rfm[\"profit\"] = 6 * rfm[\"buyers\"] - 0.5 * rfm[\"customers\"]\n",
    "rfm_prof = rfm[rfm[\"profit\"] > 0]\n",
    "\n",
    "rfm_mailed = rfm_prof[\"customers\"].sum()\n",
    "rfm_buyers = rfm_prof[\"buyers\"].sum()\n",
    "rfm_profit = 6 * rfm_buyers - 0.5 * rfm_mailed\n",
    "rfm_rome = rfm_profit / (0.5 * rfm_mailed)\n",
    "\n",
    "# --------------------\n",
    "# (2) BASELINE LOGIT\n",
    "# --------------------\n",
    "features_base = [\"purch\", \"last\", \"total_spent\"] + \\\n",
    "                [c for c in df.columns if c.startswith(\"gender_\")]\n",
    "\n",
    "Xb = df[features_base]\n",
    "y = df[\"buyer\"]\n",
    "\n",
    "logit_base = LogisticRegression(max_iter=1000)\n",
    "logit_base.fit(Xb, y)\n",
    "\n",
    "df[\"p_base\"] = logit_base.predict_proba(Xb)[:, 1]\n",
    "mask = df[\"p_base\"] >= cutoff\n",
    "\n",
    "base_mailed = mask.sum()\n",
    "base_buyers = df.loc[mask, \"buyer\"].sum()\n",
    "base_profit = 6 * base_buyers - 0.5 * base_mailed\n",
    "base_rome = base_profit / (0.5 * base_mailed)\n",
    "\n",
    "# --------------------\n",
    "# (3) ENHANCED LOGIT (interaction)\n",
    "# --------------------\n",
    "formula = (\n",
    "    \"buyer ~ purch + last + total_spent + gender_M + art_flag + gender_M * art_flag\"\n",
    ")\n",
    "\n",
    "logit_enh = smf.logit(formula=formula, data=df).fit(disp=False)\n",
    "df[\"p_enh\"] = logit_enh.predict(df)\n",
    "\n",
    "mask = df[\"p_enh\"] >= cutoff\n",
    "enh_mailed = mask.sum()\n",
    "enh_buyers = df.loc[mask, \"buyer\"].sum()\n",
    "enh_profit = 6 * enh_buyers - 0.5 * enh_mailed\n",
    "enh_rome = enh_profit / (0.5 * enh_mailed)\n",
    "\n",
    "# --------------------\n",
    "# Comparison table\n",
    "# --------------------\n",
    "comparison = pd.DataFrame({\n",
    "    \"Strategy\": [\n",
    "        \"RFM targeting\",\n",
    "        \"Logit baseline (purch + last + total_spent + gender)\",\n",
    "        \"Enhanced logit (with interaction)\"\n",
    "    ],\n",
    "    \"# Mailed\": [rfm_mailed, base_mailed, enh_mailed],\n",
    "    \"# Buyers Mailed\": [rfm_buyers, base_buyers, enh_buyers],\n",
    "    \"Profit\": [rfm_profit, base_profit, enh_profit],\n",
    "    \"ROME\": [rfm_rome, base_rome, enh_rome]\n",
    "})\n",
    "\n",
    "print(comparison)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1866b905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nThe refined logistic model uses recency (last), frequency (purch), and monetary value (total_spent) \\nbecause recently active, frequent, and high-value customers are more engaged and more likely to respond \\nto a catalog mailing, while past purchasing behavior anchors the prediction in observed demand. \\nAdding art affinity and a gender dummy aligns the model with the Florence book offer, which is closely \\ntied to art interests and may appeal differently across demographic groups. The interaction between \\ngender and art captures heterogeneous effects by allowing prior art interest to influence purchase \\nprobability differently by gender, improving targeting efficiency without substantially increasing \\nmodel complexity. In the results, the enhanced logistic model mails only 17,296 customers yet reaches \\n3,225 buyers, generating the highest profit of $10,702 and the highest ROME of 1.24. By comparison, \\nthe baseline logistic earns $8,684 (ROME 0.78) and RFM targeting earns $7,912 (ROME 0.65), showing \\nclear gains from model refinement. A key limitation is that these results are in-sample, so a natural \\nnext step would be cross-validation or an out-of-sample test to confirm that the profit and ROME \\nimprovements persist when the model is applied to new customers.\\n'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 2.F\n",
    "\n",
    "''' \n",
    "The refined logistic model uses recency (last), frequency (purch), and monetary value (total_spent) \n",
    "because recently active, frequent, and high-value customers are more engaged and more likely to respond \n",
    "to a catalog mailing, while past purchasing behavior anchors the prediction in observed demand. \n",
    "Adding art affinity and a gender dummy aligns the model with the Florence book offer, which is closely \n",
    "tied to art interests and may appeal differently across demographic groups. The interaction between \n",
    "gender and art captures heterogeneous effects by allowing prior art interest to influence purchase \n",
    "probability differently by gender, improving targeting efficiency without substantially increasing \n",
    "model complexity. In the results, the enhanced logistic model mails only 17,296 customers yet reaches \n",
    "3,225 buyers, generating the highest profit of $10,702 and the highest ROME of 1.24. By comparison, \n",
    "the baseline logistic earns $8,684 (ROME 0.78) and RFM targeting earns $7,912 (ROME 0.65), showing \n",
    "clear gains from model refinement. A key limitation is that these results are in-sample, so a natural \n",
    "next step would be cross-validation or an out-of-sample test to confirm that the profit and ROME \n",
    "improvements persist when the model is applied to new customers.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
